# Internal Linking AI Agent (SEO-Safe, Production-Ready)

A production-ready Internal Linking AI Agent that analyzes a website,
identifies contextual internal linking opportunities, and generates
professional recommendations (CSV + PDF).

This system **does NOT modify websites automatically**.
It is designed for **SEO safety**, **manual control**, and **professional use**.

---

## ğŸ” What This Agent Does

1. **Crawls** all internal pages of a given website (with rate-limiting)
2. **Extracts** clean, meaningful content (no nav/footer/script noise)
3. **Analyzes** semantic similarity using sentence transformers
4. **Clusters** pages by topic using K-means with silhouette validation
5. **Identifies** pillar pages (comprehensive, non-utility pages)
6. **Finds** natural contextual link opportunities using POS-tagged noun phrases
7. **Validates** anchors against target page content (2+ word semantic overlap)
8. **Generates** professional reports (CSV + PDF)

---

## ğŸ›¡ï¸ What This Agent Does NOT Do

- âŒ No automatic changes to any CMS
- âŒ No auto-insertion of links
- âŒ No CMS credentials required
- âŒ No JavaScript-based DOM manipulation
- âŒ No keyword stuffing
- âŒ No links from utility pages (privacy, terms, contact)

**Safe for:**
- Client audits
- Portfolio demonstrations
- Agency workflows
- SEO experiments

---

## ğŸ§  Why Internal Linking Matters

Internal linking improves:
- **Crawlability**: Helps search engines discover pages
- **Indexation Speed**: Signals page importance
- **Topical Authority**: Reinforces topic clusters (pillar â†’ supporting)
- **Link Equity Distribution**: Flows PageRank through site structure
- **Rankings**: Often drives ranking improvements within weeks

This agent focuses on **contextual, semantic links**, not random keyword stuffing.

---

## ğŸ—ï¸ Architecture Overview

```
User Website URL
    â†“
Website Crawler (with rate-limiting)
    â†“
Content Extraction (removes noise)
    â†“
Semantic Embeddings (sentence-transformers)
    â†“
Clustering & Pillar Identification
    â†“
Anchor Extraction (POS tagging, validation)
    â†“
CSV + PDF Report
```

**Integration with n8n:**
```
WordPress Form
    â†“
n8n Webhook
    â†“
Python Agent (this repo)
    â†“
Timestamped CSV/PDF/JSON Output
```

---

## ğŸ“ Project Structure

```
internal_links_ai-agent/
â”‚
â”œâ”€â”€ run_agent.py                 # Main entry point (handles n8n + CLI)
â”œâ”€â”€ crawler.py                   # Crawls all internal pages
â”œâ”€â”€ content_extractor.py         # Cleans & extracts page content
â”œâ”€â”€ semantic_topics.py           # Embeddings + clustering
â”œâ”€â”€ semantic_graph.py            # Cluster grouping
â”œâ”€â”€ internal_link_planner.py     # Link planning logic (safety rules)
â”œâ”€â”€ output_writer.py             # CSV output (dynamic filenames)
â”œâ”€â”€ pdf_report.py                # Professional PDF generation
â”œâ”€â”€ url_utils.py                 # URL normalization
â”‚
â”œâ”€â”€ test_internal_linking.py     # Comprehensive test suite (17 tests)
â”œâ”€â”€ requirements.txt             # Dependencies
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

### 1. Clone the repository
```bash
git clone https://github.com/yourusername/internal-linking-ai-agent.git
cd internal-linking-ai-agent
```

### 2. Create virtual environment (optional)
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

Dependencies installed:
- **requests** / **beautifulsoup4** / **lxml**: Web scraping
- **pandas**: Data handling
- **sentence-transformers**: Semantic embeddings
- **scikit-learn**: K-means clustering + silhouette scoring
- **nltk**: POS tagging for anchor extraction
- **reportlab**: PDF generation

---

## ğŸš€ Usage

### CLI Usage (Local Development)

**Basic run:**
```bash
python run_agent.py --site https://example.com
```

**Output:**
```
============================================================
Internal Linking Analysis: https://example.com
============================================================
Status: SUCCESS
Pages crawled: 47
Usable content pages: 34
Clusters: 5
Cohesion score: 0.423
Links recommended: 12

[!] Warnings:
  â€¢ Low cluster cohesion (0.159). Results may be less topically relevant.

[OK] Outputs:
  â€¢ csv: example_com_20251224_180212_links.csv
  â€¢ pdf: example_com_20251224_180212_report.pdf
  â€¢ metadata: example_com_20251224_180212_metadata.json
============================================================
```

**Advanced options:**
```bash
# Skip PDF generation (faster for testing)
python run_agent.py --site https://example.com --skip-pdf

# JSON output only (for n8n integration)
python run_agent.py --site https://example.com --json-output

# CSV only
python run_agent.py --site https://example.com --format csv
```

---

### n8n Integration

Add a **webhook node** in n8n to call the agent:

```bash
curl -X POST http://your-vps:5000/webhook \
  -H "Content-Type: application/json" \
  -d '{"site": "https://example.com"}'
```

The agent responds with JSON:
```json
{
  "status": "success",
  "site": "https://example.com",
  "timestamp": "2025-12-24T18:02:12.188405",
  "warnings": [],
  "metadata": {
    "total_pages_crawled": 47,
    "usable_pages": 34,
    "num_clusters": 5,
    "silhouette_score": 0.423,
    "num_links_recommended": 12
  },
  "outputs": {
    "csv": "example_com_20251224_180212_links.csv",
    "pdf": "example_com_20251224_180212_report.pdf",
    "metadata": "example_com_20251224_180212_metadata.json"
  }
}
```

---

## ğŸ§ª Testing

Run the comprehensive test suite:
```bash
python test_internal_linking.py
```

**Tests included (17 total):**
- âœ… Utility page detection (privacy, terms, contact, cookie, login)
- âœ… Anchor validation (must exist in target, semantic overlap â‰¥2 words)
- âœ… Pillar identification (longest non-utility page per cluster)
- âœ… Link generation safety (no self-links, no utility page sources)
- âœ… Cluster quality (silhouette score calculation)
- âœ… Output format (CSV structure, no duplicates)

**Expected output:**
```
Ran 17 tests in 2.101s
OK
```

---

## ğŸ“Š Understanding the Output

### CSV Format
```
from,to,anchor,sentence
https://example.com/page-a,https://example.com/page-b,"anchor text","Full sentence context..."
```

- **from**: Source page (where the link will be added)
- **to**: Target page (where the link points)
- **anchor**: Exact text to use as link (already exists on source page)
- **sentence**: Context (for verification)

### PDF Report
Professional, client-ready document including:
- Executive summary
- Methodology explanation
- Cluster cohesion metrics
- Detailed recommendations table
- Implementation guide

### Metadata JSON
```json
{
  "status": "success",
  "site": "https://example.com",
  "timestamp": "2025-12-24T18:02:12.188405",
  "metadata": {
    "total_pages_crawled": 47,
    "usable_pages": 34,
    "num_clusters": 5,
    "silhouette_score": 0.423,
    "num_links_recommended": 12
  },
  "warnings": [],
  "errors": []
}
```

---

## ğŸ”’ Safety Guarantees

This system enforces strict SEO safety:

1. **No Utility Pages**: Links never originate from privacy, terms, contact, cookie, login pages
2. **Anchor Validation**: Anchors must have 2+ word semantic overlap with target
3. **No Self-Links**: Pages never link to themselves
4. **Max 1 Link Per Topic**: Only one link per source page per cluster
5. **Semantic Basis**: Links reinforce topical authority, not random keywords
6. **POS-Filtered Anchors**: Only grammatically sound noun phrases extracted
7. **Cluster Validation**: Silhouette score shows topical cohesion
8. **Manual Control**: All recommendations require human approval before implementation

---

## ğŸš¢ Deployment (VPS + n8n)

### Prerequisites
- Python 3.8+
- pip package manager
- Git

### Step 1: Clone on VPS
```bash
cd /var/www
git clone https://github.com/yourusername/internal-linking-ai-agent.git
cd internal-linking-ai-agent
```

### Step 2: Install Dependencies
```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Step 3: Create Wrapper Script (for n8n)
Save as `agent.sh`:
```bash
#!/bin/bash
cd /var/www/internal-linking-ai-agent
source venv/bin/activate
python run_agent.py --site "$1" --json-output
```

Make executable:
```bash
chmod +x agent.sh
```

### Step 4: Configure n8n
In n8n, create a webhook that calls:
```bash
/var/www/internal-linking-ai-agent/agent.sh https://example.com
```

### Step 5: Output Management
Set up cron job to archive old outputs:
```bash
# Archive outputs older than 30 days
0 2 * * * find /var/www/internal-linking-ai-agent -name "*_links.csv" -mtime +30 -exec gzip {} \;
```

---

## ğŸ“ˆ Performance Notes

- **Crawl time**: ~30 sec - 5 min depending on site size (includes rate-limiting)
- **Embedding time**: ~5-30 sec depending on page count
- **PDF generation**: ~2-5 sec
- **Total pipeline**: 1-10 minutes for typical sites

For large sites (200+ pages), consider running overnight or in batches.

---

## ğŸ› Troubleshooting

| Issue | Solution |
|---|---|
| `ModuleNotFoundError: nltk` | Run `pip install -r requirements.txt` |
| `No pages crawled` | Check site URL, firewall, robots.txt |
| `Only 1 usable page` | Site may have thin content (<200 words) |
| `Low silhouette score (< 0.3)` | Pages may be poorly topically organized |
| `No links recommended` | Pages may be too similar or under-linked |
| `PDF generation fails` | Check write permissions in output directory |

---

## ğŸ¤ Contributing

This is a production system. Before submitting changes:

1. Run the test suite: `python test_internal_linking.py`
2. Test locally: `python run_agent.py --site https://yoursite.com`
3. Verify CSV, PDF, and JSON outputs
4. Check error handling with invalid URLs

---

## ğŸ“ License

This project is built for professional SEO automation. Use it responsibly.

---

## ğŸ¯ Roadmap

- [ ] Database storage of historical reports
- [ ] Competitor analysis module
- [ ] A/B testing suggestions
- [ ] Bulk site analysis
- [ ] WordPress plugin wrapper
- [ ] REST API (FastAPI)

---

## ğŸ“§ Support

Questions? Issues?
- Check troubleshooting section
- Review test suite for expected behavior
- Check logs in `.log` files

---

**Built with â¤ï¸ for SEO professionals who respect the web.**

pip install -r requirements.txt

â–¶ï¸ How to Run (Local Test)
python run_agent.py --site https://example.com

Output:

output/internal_links.csv

output/internal-linking-report.pdf

ğŸ”Œ n8n Integration (Production Use)

This project is designed to be executed via n8n Execute Command.

Example n8n command:

python /opt/internal-linking-ai-agent/run_agent.py --site {{$json.site_url}}


This allows:

WordPress â†’ n8n â†’ Python execution

Automated report generation

Portfolio-safe backend processing

ğŸ“„ Output Report Includes

Website overview

Total pages analyzed

Suggested internal links

Source page

Target page

Suggested anchor text

Context sentence

Implementation guidelines

ğŸš¦ SEO Safety Principles

Crawl limits enforced

Context-only anchors

Max link suggestions per page

No repeated anchors

Manual review encouraged

This agent follows white-hat SEO best practices.

ğŸ§© Future Enhancements

AI-assisted anchor text selection

Page importance scoring

Topical authority mapping

Google Search Console integration

Ranking impact tracking

ğŸ‘¤ Author

Built by Mehreen Siraj
SEO Automation & AI Systems

Portfolio: https://mehreensiraj.com

ğŸ“œ License

MIT License
Use, modify, and adapt freely.


---

## âœ… What You Should Do Next (Exact Order)

1ï¸âƒ£ Add these two files to your repo  
2ï¸âƒ£ Commit & push to GitHub  
3ï¸âƒ£ Clone repo on your VPS  
4ï¸âƒ£ Install requirements on VPS  
5ï¸âƒ£ Confirm `run_agent.py` runs on VPS  

ğŸ‘‰ **Do NOT touch n8n again until step 5 is confirmed.**

---

When youâ€™re ready, say **â€œrepo pushedâ€**  
Next, Iâ€™ll:
- Review `run_agent.py`
- Add **crawl limits + robots respect**
- Design the **PDF layout (this matters a LOT)**




